{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import pandas as pd\n",
    "# from sklearn.metrics import precision_recall_curve, auc, roc_curve, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data = pd.read_csv('/home/bram/bramenv/matrices/finalmatrix.csv')\n",
    "# y_data_pre = pd.read_csv('/home/bram/bramenv/matrices/full_annotations_ordered2.csv')\n",
    "\n",
    "matrix = pd.read_csv('/home/bram/bramenv/aa_test_2 matrix/newmatrix_final.csv')\n",
    "\n",
    "# Drop the 'annot' column to create X_data\n",
    "X_data = matrix.drop('annot', axis=1)\n",
    "\n",
    "# Extract the 'BDSPPatientID' and 'annot' columns to create y_data_pre\n",
    "y_data_pre = matrix[['BDSPPatientID', 'annot']]\n",
    "\n",
    "print(X_data)\n",
    "print(y_data_pre)\n",
    "assert len(X_data) == len(y_data_pre), \"DataFrames must have the same length\"\n",
    "X_data['annot']=y_data_pre['annot']\n",
    "print(X_data)\n",
    "\n",
    "X_data=X_data.sample(frac=1, random_state=2023)\n",
    "y_data = X_data[['BDSPPatientID', 'annot', 'hospital']]\n",
    "print(y_data)\n",
    "y_data_pre=y_data\n",
    "print(y_data_pre)\n",
    "y = y_data_pre['annot']\n",
    "print(y)\n",
    "\n",
    "X_data=X_data.drop(['annot'], axis=1)\n",
    "X=X_data.drop(['BDSPPatientID', 'CreateDate', 'hospital'], axis=1) #dopamine and dopamineagonist?\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['ICD'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data['hospital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hospitals = ['BIDMC', 'MGB']\n",
    "test_hospital = 'STF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data['hospital'].isin(train_hospitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[y_data['hospital'].isin(train_hospitals)]\n",
    "y_train = y[y_data['hospital'].isin(train_hospitals)]\n",
    "X_test = X[y_data['hospital'] == test_hospital]\n",
    "y_test = y[y_data['hospital'] == test_hospital]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "patient_ids = y_data_pre['BDSPPatientID'][y_data['hospital'] == test_hospital]\n",
    "print(patient_ids)\n",
    "patient_ids=patient_ids.reset_index(drop=True)\n",
    "print(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_predictions_nonbin=[]\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_row_numbers = []\n",
    "\n",
    "# outer loop\n",
    "auc_cv = []\n",
    "auc_pr = []\n",
    "f1_cv = []\n",
    "precision_sc=[]\n",
    "recall_sc=[]\n",
    "accuracy_sc=[]\n",
    "cf_cv = []\n",
    "final_Cs = []\n",
    "final_l1 = []\n",
    "predictions = []\n",
    "roc_curves = []\n",
    "pr_curves = []\n",
    "\n",
    "# Dictionary to store feature importances\n",
    "feature_importances_dict = {feature: [] for feature in X.columns}\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=2023,\n",
    "    n_jobs=30\n",
    ")\n",
    "\n",
    "search_spaces = {\n",
    "    'n_estimators': (10, 200),\n",
    "    'max_depth': (2, 10),\n",
    "    'min_samples_split': (2, 20),\n",
    "    'min_samples_leaf': (1, 20),\n",
    "}\n",
    "\n",
    "model_cv = BayesSearchCV(\n",
    "    model,\n",
    "    search_spaces,\n",
    "    n_iter=50,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=47,\n",
    "    cv=10,\n",
    "    random_state=2023\n",
    ")\n",
    "\n",
    "model_cv.fit(X_train, y_train)\n",
    "\n",
    "# #Find the Best hyperparameters and append them\n",
    "# best_hparams = model_cv.best_params_\n",
    "# best_C = best_hparams['C']\n",
    "# best_l1_ratio = best_hparams['l1_ratio']\n",
    "# final_Cs.append(best_C)\n",
    "# final_l1.append(best_l1_ratio)\n",
    "\n",
    "model = model_cv.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "ytr_pred = model.predict_proba(X_train)[:,1]      # Xtr is training features, ytr is training labels, ytr_pred is training predictions = Prob(y=1|Xtr)\n",
    "yte_pred = model.predict_proba(X_test)[:,1]    # Xte is testing features\n",
    "\n",
    "fpr, tpr, cutoffs = roc_curve(y_train, ytr_pred) # changed this one to training\n",
    "best_cutoff = cutoffs[np.argmax(tpr - fpr)]  # get best cut off from training set\n",
    "yte_pred_bin =(yte_pred>best_cutoff).astype(int)\n",
    "predictions.append(yte_pred_bin)\n",
    "\n",
    "model_filename = 'model_train_MGB_BIDMC_test_Stanford_Notes+ICD+Med_best_model.pickle'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump({'model':model, 'cutoff':best_cutoff}, f)\n",
    "\n",
    "fpr, tpr, cutoffs = roc_curve(y_test, yte_pred) # repeat to get the fpr and tpr from testing set\n",
    "\n",
    "auc_cv=roc_auc_score(y_test, yte_pred)\n",
    "f1_cv=f1_score(y_test, yte_pred_bin)\n",
    "cf_cv=confusion_matrix(y_test, yte_pred_bin)\n",
    "precision_sc=precision_score(y_test, yte_pred_bin)\n",
    "recall_sc=recall_score(y_test, yte_pred_bin)\n",
    "accuracy_sc = accuracy_score(y_test, yte_pred_bin)\n",
    "\n",
    "# Store the predictions and true labels for this fold\n",
    "all_predictions_nonbin.extend(yte_pred)\n",
    "all_predictions.extend(yte_pred_bin)\n",
    "all_true_labels.extend(y_test)\n",
    "all_row_numbers.extend(y_data[y_data['hospital'] == test_hospital]['BDSPPatientID'])\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, yte_pred)\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "# Save info for the plots\n",
    "roc_curves.append((fpr, tpr, roc_auc_score(y_test, yte_pred)))\n",
    "pr_curves.append((recall, precision, auc_pr))\n",
    "# auc_pr_loop = auc(recall, precision)\n",
    "# auc_pr.append(auc_pr_loop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract feature importances (coefficients)\n",
    "feature_importances = model.feature_importances_\n",
    "for feature, importance in zip(X.columns, feature_importances):\n",
    "    feature_importances_dict[feature].append(importance)\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)\n",
    "\n",
    "print(auc_cv)\n",
    "print(auc_pr)\n",
    "\n",
    "all_true_labels = pd.Series(all_true_labels)\n",
    "all_predictions = pd.Series(all_predictions)\n",
    "\n",
    "false_positive_ids = patient_ids[(all_true_labels == 0) & (all_predictions == 1)]\n",
    "false_negative_ids = patient_ids[(all_true_labels == 1) & (all_predictions == 0)]\n",
    "false_positives.append(false_positive_ids)\n",
    "false_negatives.append(false_negative_ids)\n",
    "# Save false positives and false negatives to CSV files\n",
    "false_positives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fp) for fp in false_positives]), 'BDSPPatientID': np.concatenate(false_positives)})\n",
    "false_negatives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fn) for fn in false_negatives]), 'BDSPPatientID': np.concatenate(false_negatives)})\n",
    "\n",
    "false_positives_df.to_csv('false_positives.csv', index=False)\n",
    "false_negatives_df.to_csv('false_negatives.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_predictions)\n",
    "print(all_true_labels)\n",
    "print(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "feature_importance_df.to_csv(f'feat_importances.csv', index=False)\n",
    "\n",
    "# Aggregate feature importances by averaging over folds\n",
    "avg_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importances_dict.items()}\n",
    "sorted_features = sorted(avg_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "features, importances = zip(*sorted_features)\n",
    "\n",
    "# Select top 10 and bottom 10 features\n",
    "top_features = features[:15]\n",
    "top_importances = importances[:15]\n",
    "bottom_features = features[-5:]\n",
    "bottom_importances = importances[-5:]\n",
    "\n",
    "# Combine top and bottom features for plotting\n",
    "plot_features = top_features + bottom_features\n",
    "plot_importances = top_importances + bottom_importances\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(plot_features, plot_importances, color='skyblue')\n",
    "plt.xlabel('Average Coefficient Value')\n",
    "plt.title('Top 10 and Bottom 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'feat_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom feature names\n",
    "custom_plot_features = [\n",
    "   \"Parkinson+\",\"Parkinson's Disease+\", \"332.0 / G20\", \n",
    "    'Levodopa',\"Carbidopa\",'Disease+',\"Tremor+\",\"Bradykinesia+\",\"PD+\", \n",
    "    \"Rasagiline\", \"DBS+\",\n",
    "    'Rigidity+', \"Freezing+\", 'Parkinsonism+', \n",
    "     \"Entacapone\", \"RequipXL\", \"Apokyn\", \n",
    "    \"Tasmar\", 'MAO', 'Istradefylline'\n",
    "]\n",
    "print(len(custom_plot_features))\n",
    "# Ensure the length of custom features matches the length of plot_features\n",
    "assert len(custom_plot_features) == len(plot_features), \"The number of custom features must match the number of plot features.\"\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(plot_features)), plot_importances, color='skyblue')\n",
    "plt.xlabel('Average Coefficient Value')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "\n",
    "# Set custom y-axis labels\n",
    "plt.yticks(range(len(plot_features)), custom_plot_features)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves overlayed on a single graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# plt.plot(fpr, tpr, label='AUC = {:.4f}'.format(auc_score))\n",
    "# youden = np.max(tpr-fpr)\n",
    "# plt.scatter(fpr[np.argmax(tpr-fpr)], tpr[np.argmax(tpr-fpr)], c='red', label=f'Youden\\'s Index = {youden:.4f}')\n",
    "for fpr, tpr, auc_score in roc_curves:\n",
    "    plt.plot(fpr, tpr, label='AUC = {:.4f}'.format(auc_score))\n",
    "    youden = np.max(tpr-fpr)\n",
    "    plt.scatter(fpr[np.argmax(tpr-fpr)], tpr[np.argmax(tpr-fpr)], c='red', label=f'Youden\\'s Index = {youden:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classifier\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f'AUC_iter.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot all PR curves overlayed on a single graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.plot(recall, precision, label='PR Curve (AUC = {:.4f})'.format(auc_pr_loop))\n",
    "# for recall, precision, auc_pr_loop in pr_curves:\n",
    "#     plt.plot(recall, precision, label='PR Curve (AUC = {:.4f})'.format(auc_pr_loop))\n",
    "for recall, precision, auc_pr_loop in pr_curves:\n",
    "    if isinstance(auc_pr_loop, list):\n",
    "        if len(auc_pr_loop) > 0:\n",
    "            auc_pr_loop = float(auc_pr_loop[0])  # Adjust as necessary to extract the float\n",
    "        else:\n",
    "            raise ValueError(\"auc_pr_loop list is empty.\")\n",
    "    elif not isinstance(auc_pr_loop, float):\n",
    "        raise TypeError(\"auc_pr_loop must be a float or convertible to a float.\")\n",
    "    plt.plot(recall, precision, label='PR Curve (AUC = {:.4f})'.format(auc_pr_loop))\n",
    "\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig(f'PR_iter.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'Unnamed: 0': all_row_numbers, 'annot': all_true_labels, 'prediction BINARY': all_predictions, 'prediction': all_predictions_nonbin})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv(f'LR_iter_pred.csv', index=False)\n",
    "\n",
    "# Calculate the final AUC and F1\n",
    "auc_final = np.mean(auc_cv)\n",
    "f1_final = np.mean(f1_cv)\n",
    "\n",
    "# Save all the data\n",
    "df = pd.DataFrame()\n",
    "df['auc'] = auc_cv\n",
    "df['f1s'] = f1_cv\n",
    "df['auc_pr'] = auc_pr\n",
    "df['C'] = final_Cs\n",
    "df['l1_ratio'] = final_l1\n",
    "df_pred = pd.DataFrame()\n",
    "df1 = pd.DataFrame(predictions[0])\n",
    "df_pred = pd.concat([df1]).reset_index()\n",
    "df.to_csv(f'LR_iter.csv')\n",
    "\n",
    "# Print final Data\n",
    "print(auc_final)\n",
    "print(auc_pr)\n",
    "print(f1_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all confusion matrices to get overall counts\n",
    "overall_cf_matrix = np.sum(cf_cv, axis=0)\n",
    "print(\"Overall Confusion Matrix:\\n\", overall_cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cf_cv, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Overall Confusion Matrix on Holdout Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig('overall_confusion_matrix_holdout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_metrics(cf_matrix):\n",
    "#     TN, FP, FN, TP = cf_matrix.ravel()\n",
    "#     accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "#     precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "#     recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "#     return accuracy, precision, recall, f1\n",
    "\n",
    "# # Calculate metrics for each fold\n",
    "# for fold_index, cf_matrix in enumerate(cf_cv):\n",
    "#     accuracy, precision, recall, f1 = calculate_metrics(cf_matrix)\n",
    "#     print(f\"Metrics for Fold {fold_index + 1}:\")\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# # Calculate overall metrics\n",
    "# overall_cf_matrix = np.sum(cf_cv, axis=0)\n",
    "# overall_accuracy, overall_precision, overall_recall, overall_f1 = calculate_metrics(overall_cf_matrix)\n",
    "# print(\"Overall Metrics:\")\n",
    "# print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "# print(f\"Precision: {overall_precision:.4f}\")\n",
    "# print(f\"Recall: {overall_recall:.4f}\")\n",
    "# print(f\"F1 Score: {overall_f1:.4f}\")\n",
    "def calculate_metrics(cf_matrix):\n",
    "    TN, FP, FN, TP = cf_matrix.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "overall_accuracy_holdout, overall_precision_holdout, overall_recall_holdout, overall_f1_holdout = calculate_metrics(cf_cv)\n",
    "print(\"Overall Metrics on Holdout Set:\")\n",
    "print(f\"Accuracy: {overall_accuracy_holdout:.4f}\")\n",
    "print(f\"Precision: {overall_precision_holdout:.4f}\")\n",
    "print(f\"Recall: {overall_recall_holdout:.4f}\")\n",
    "print(f\"F1 Score: {overall_f1_holdout:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test=y_test.drop('index', axis=1)\n",
    "y_test=y_test.reset_index(drop=True)\n",
    "\n",
    "print(y_test)\n",
    "print(yte_pred)\n",
    "print(yte_pred_bin)\n",
    "yte_pred_array=np.array(yte_pred)\n",
    "yte_pred_bin_array=np.array(yte_pred_bin)\n",
    "print(yte_pred_array)\n",
    "print(yte_pred_bin_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resample(y_true, y_pred_proba, y_pred_bin, n_iterations=800, alpha=0.95):\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "    auc_prs = []\n",
    "    roc_curves = []\n",
    "    pr_curves = []\n",
    "    \n",
    "    n_size = len(y_true)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.randint(0, n_size, n_size)\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue  # skip this resample if it does not have both classes\n",
    "        \n",
    "        aucs.append(roc_auc_score(y_true[indices], y_pred_proba[indices]))\n",
    "        f1s.append(f1_score(y_true[indices], y_pred_bin[indices]))\n",
    "        precisions.append(precision_score(y_true[indices], y_pred_bin[indices]))\n",
    "        recalls.append(recall_score(y_true[indices], y_pred_bin[indices]))\n",
    "        accuracies.append(accuracy_score(y_true[indices], y_pred_bin[indices]))\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_true[indices], y_pred_proba[indices])\n",
    "        auc_prs.append(auc(recall, precision))\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_pred_proba[indices])\n",
    "        roc_curves.append((fpr, tpr))\n",
    "        \n",
    "        pr_curves.append((precision, recall))\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_p = ((1.0 - alpha) / 2.0) * 100\n",
    "    upper_p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n",
    "    \n",
    "    auc_ci = np.percentile(aucs, [lower_p, upper_p])\n",
    "    f1_ci = np.percentile(f1s, [lower_p, upper_p])\n",
    "    precision_ci = np.percentile(precisions, [lower_p, upper_p])\n",
    "    recall_ci = np.percentile(recalls, [lower_p, upper_p])\n",
    "    accuracy_ci = np.percentile(accuracies, [lower_p, upper_p])\n",
    "    auc_pr_ci = np.percentile(auc_prs, [lower_p, upper_p])\n",
    "    \n",
    "    return auc_ci, f1_ci, precision_ci, recall_ci, accuracy_ci, auc_pr_ci, roc_curves, pr_curves\n",
    "\n",
    "# Perform bootstrap resampling to get confidence intervals\n",
    "auc_ci, f1_ci, precision_ci, recall_ci, accuracy_ci, auc_pr_ci, roc_curves_bootstrap, pr_curves_bootstrap = bootstrap_resample(y_test, yte_pred_array, yte_pred_bin_array)\n",
    "\n",
    "# Print metrics and confidence intervals\n",
    "print(\"Metrics on Holdout Set:\")\n",
    "print(f\"AUC: {auc_cv:.4f} (95% CI: {auc_ci[0]:.4f} - {auc_ci[1]:.4f})\")\n",
    "print(f\"F1 Score: {f1_cv:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})\")\n",
    "print(f\"Precision: {precision_sc:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})\")\n",
    "print(f\"Recall: {recall_sc:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})\")\n",
    "print(f\"Accuracy: {accuracy_sc:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})\")\n",
    "print(f\"AUC-PR: {auc_pr:.4f} (95% CI: {auc_pr_ci[0]:.4f} - {auc_pr_ci[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_intervals(curves, n_bins=100):\n",
    "    mean_curve = np.linspace(0, 1, n_bins)\n",
    "    interpolated_curves = [np.interp(mean_curve, curve[0], curve[1]) for curve in curves]\n",
    "    lower_ci = np.percentile(interpolated_curves, 2.5, axis=0)\n",
    "    upper_ci = np.percentile(interpolated_curves, 97.5, axis=0)\n",
    "    mean_curve = np.mean(interpolated_curves, axis=0)\n",
    "    return mean_curve, lower_ci, upper_ci\n",
    "\n",
    "# Calculate confidence intervals for ROC\n",
    "roc_fpr_interp = np.linspace(0, 1, 100)\n",
    "roc_tpr_interp = [np.interp(roc_fpr_interp, fpr, tpr) for fpr, tpr in roc_curves_bootstrap]\n",
    "roc_mean_tpr = np.mean(roc_tpr_interp, axis=0)\n",
    "roc_lower_tpr = np.percentile(roc_tpr_interp, 2.5, axis=0)\n",
    "roc_upper_tpr = np.percentile(roc_tpr_interp, 97.5, axis=0)\n",
    "\n",
    "# Calculate confidence intervals for PR\n",
    "pr_recall_interp = np.linspace(0, 1, 100)\n",
    "pr_precision_interp = [np.interp(pr_recall_interp, recall[::-1], precision[::-1]) for precision, recall in pr_curves_bootstrap]\n",
    "pr_mean_precision = np.mean(pr_precision_interp, axis=0)\n",
    "pr_lower_precision = np.percentile(pr_precision_interp, 2.5, axis=0)\n",
    "pr_upper_precision = np.percentile(pr_precision_interp, 97.5, axis=0)\n",
    "\n",
    "# Plot ROC curve with confidence intervals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(roc_curves[0][0], roc_curves[0][1], label=f'Main ROC (AUC = {roc_curves[0][2]:.4f})', color='blue')\n",
    "plt.fill_between(roc_fpr_interp, roc_lower_tpr, roc_upper_tpr, color='blue', alpha=0.2, label='95% CI')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve on Holdout Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('AUC_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot PR curve with confidence intervals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(pr_curves[0][0], pr_curves[0][1], label=f'Main PR Curve (AUC = {pr_curves[0][2]:.4f})', color='blue')\n",
    "plt.fill_between(pr_recall_interp, pr_lower_precision, pr_upper_precision, color='blue', alpha=0.2, label='95% CI')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve on Holdout Set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig('PR_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Save all the data\n",
    "df_holdout = pd.DataFrame({\n",
    "    'metric': ['auc', 'f1', 'precision', 'recall', 'accuracy', 'auc_pr'],\n",
    "    'value': [auc_cv, f1_cv, precision_sc, recall_sc, accuracy_sc, auc_pr],\n",
    "    'ci_lower': [auc_ci[0], f1_ci[0], precision_ci[0], recall_ci[0], accuracy_ci[0], auc_pr_ci[0]],\n",
    "    'ci_upper': [auc_ci[1], f1_ci[1], precision_ci[1], recall_ci[1], accuracy_ci[1], auc_pr_ci[1]]\n",
    "})\n",
    "df_holdout.to_csv('LR_holdout_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bramenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

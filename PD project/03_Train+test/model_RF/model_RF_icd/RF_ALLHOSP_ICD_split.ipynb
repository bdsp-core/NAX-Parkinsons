{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, confusion_matrix, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv('/home/bram/Project_PD/aa_test_2 matrix/training_set_final.csv')\n",
    "\n",
    "# Drop the 'annot' column to create X_data\n",
    "X_data = matrix.drop('annot', axis=1)\n",
    "\n",
    "# Extract the 'BDSPPatientID' and 'annot' columns to create y_data_pre\n",
    "y_data_pre = matrix[['BDSPPatientID', 'annot']]\n",
    "\n",
    "print(X_data)\n",
    "print(y_data_pre)\n",
    "\n",
    "print(X_data)\n",
    "print(y_data_pre)\n",
    "assert len(X_data) == len(y_data_pre), \"DataFrames must have the same length\"\n",
    "X_data['annot']=y_data_pre['annot']\n",
    "print(X_data)\n",
    "\n",
    "# X_data=X_data.sample(frac=1, random_state=2023)\n",
    "y_data = X_data[['BDSPPatientID', 'annot']]\n",
    "print(y_data)\n",
    "y_data_pre=y_data\n",
    "print(y_data_pre)\n",
    "y = y_data_pre['annot']\n",
    "print(y)\n",
    "\n",
    "X_data=X_data.drop(['annot'], axis=1)\n",
    "X=X_data.drop(['BDSPPatientID', 'CreateDate', 'hospital'], axis=1)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DROP ALL COLUMNS BEFORE ICD COLUMN\n",
    "col_to_drop_up_to = 'ICD'\n",
    "\n",
    "col_index = X.columns.get_loc(col_to_drop_up_to)\n",
    "\n",
    "# Drop all columns up to and including the specified column\n",
    "X = X.iloc[:, col_index :]\n",
    "\n",
    "print(X.columns)\n",
    "# DROP ALL COLUMNS AFTER ICD COLUMN\n",
    "colindex2=X.columns.get_loc(col_to_drop_up_to)\n",
    "X=X.iloc[: , :colindex2+1]\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store false positive and false negative patient IDs\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "patient_ids = y_data_pre['BDSPPatientID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for storing results\n",
    "all_predictions_nonbin=[]\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_row_numbers = []\n",
    "auc_cv = []\n",
    "auc_pr = []\n",
    "f1_cv = []\n",
    "cf_cv = []\n",
    "predictions = []\n",
    "roc_curves = []\n",
    "pr_curves = []\n",
    "feature_importances_dict = {feature: [] for feature in X.columns}\n",
    "\n",
    "\n",
    "# Dictionary to store patient IDs for each fold\n",
    "fold_patient_ids = {f'fold_{i+1}': {'train': [], 'test': []} for i in range(10)}\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "\n",
    "for cvi, (train_index, test_index) in enumerate(gkf.split(X, y, groups=y_data_pre['BDSPPatientID'])):\n",
    "    Xtr, Xte = X.loc[train_index], X.loc[test_index]\n",
    "    ytr, yte = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "# Store patient IDs\n",
    "    fold_patient_ids[f'fold_{cvi+1}']['train'].extend(y_data_pre.loc[train_index, 'BDSPPatientID'].tolist())\n",
    "    fold_patient_ids[f'fold_{cvi+1}']['test'].extend(y_data_pre.loc[test_index, 'BDSPPatientID'].tolist())\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=2023,\n",
    "        n_jobs=23\n",
    "    )\n",
    "    \n",
    "    search_spaces = {\n",
    "        'n_estimators': (10, 200),\n",
    "        'max_depth': (2, 10),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 20),\n",
    "    }\n",
    "    \n",
    "    model_cv = BayesSearchCV(\n",
    "        model,\n",
    "        search_spaces,\n",
    "        n_iter=50,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=47,\n",
    "        cv=10,\n",
    "        random_state=2023\n",
    "    )\n",
    "    \n",
    "    model_cv.fit(Xtr, ytr)\n",
    "\n",
    "    model = model_cv.best_estimator_\n",
    "    \n",
    "\n",
    "    ytr_pred = model.predict_proba(Xtr)[:, 1]\n",
    "    yte_pred = model.predict_proba(Xte)[:, 1]\n",
    "\n",
    "    fpr, tpr, cutoffs = roc_curve(ytr, ytr_pred)\n",
    "    best_cutoff = cutoffs[np.argmax(tpr - fpr)]\n",
    "    yte_pred_bin = (yte_pred > best_cutoff).astype(int)\n",
    "\n",
    "    auc_cv.append(roc_auc_score(yte, yte_pred))\n",
    "    f1_cv.append(f1_score(yte, yte_pred_bin))\n",
    "    cf_cv.append(confusion_matrix(yte, yte_pred_bin))\n",
    "    predictions.append(yte_pred_bin)\n",
    "\n",
    "    model_filename = f'model_train_allhospitals_ICD+_fold{cvi+1}.pickle'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump({'model':model, 'cutoff':best_cutoff}, f)\n",
    "    \n",
    "    fpr, tpr, cutoffs = roc_curve(yte, yte_pred)\n",
    "    all_predictions_nonbin.extend(yte_pred)\n",
    "    all_predictions.extend(yte_pred_bin)\n",
    "    all_true_labels.extend(yte)\n",
    "    all_row_numbers.extend(y_data.iloc[test_index]['BDSPPatientID'])\n",
    "\n",
    "    roc_curves.append((fpr, tpr, roc_auc_score(yte, yte_pred)))\n",
    "    precision, recall, thresholds = precision_recall_curve(yte, yte_pred)\n",
    "    pr_curves.append((recall, precision, auc(recall, precision)))\n",
    "    auc_pr_loop = auc(recall, precision)\n",
    "    auc_pr.append(auc_pr_loop)\n",
    "\n",
    "    feature_importances = model.feature_importances_\n",
    "    for feature, importance in zip(X.columns, feature_importances):\n",
    "        feature_importances_dict[feature].append(importance)\n",
    "\n",
    "    print(\"Feature importances for this fold:\")\n",
    "    print(pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False))\n",
    "\n",
    "    print(auc_cv)\n",
    "    print(auc_pr)\n",
    "\n",
    "    # Convert fold_patient_ids to DataFrame and save to CSV\n",
    "fold_patient_ids_list = []\n",
    "for fold, ids in fold_patient_ids.items():\n",
    "    for train_id in ids['train']:\n",
    "        fold_patient_ids_list.append({'fold': fold, 'type': 'train', 'patient_id': train_id})\n",
    "    for test_id in ids['test']:\n",
    "        fold_patient_ids_list.append({'fold': fold, 'type': 'test', 'patient_id': test_id})\n",
    "\n",
    "fold_patient_ids_df = pd.DataFrame(fold_patient_ids_list)\n",
    "fold_patient_ids_df.to_csv('fold_patient_ids.csv', index=False)\n",
    "\n",
    "all_true_labels = pd.Series(all_true_labels)\n",
    "all_predictions = pd.Series(all_predictions)\n",
    "\n",
    "false_positive_ids = patient_ids[(all_true_labels == 0) & (all_predictions == 1)]\n",
    "false_negative_ids = patient_ids[(all_true_labels == 1) & (all_predictions == 0)]\n",
    "\n",
    "false_positives.append(false_positive_ids)\n",
    "false_negatives.append(false_negative_ids)\n",
    "# Save false positives and false negatives to CSV files\n",
    "false_positives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fp) for fp in false_positives]), 'BDSPPatientID': np.concatenate(false_positives)})\n",
    "false_negatives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fn) for fn in false_negatives]), 'BDSPPatientID': np.concatenate(false_negatives)})\n",
    "\n",
    "false_positives_df.to_csv('false_positives.csv', index=False)\n",
    "false_negatives_df.to_csv('false_negatives.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(auc_cv)) \n",
    "print(np.mean(auc_pr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate feature importances by averaging over folds\n",
    "avg_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importances_dict.items()}\n",
    "sorted_features = sorted(avg_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "features, importances = zip(*sorted_features)\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "feature_importance_df.to_csv('feat_importances.csv', index=False)\n",
    "\n",
    "# Select top 10 and bottom 10 features\n",
    "top_features = features[:1]\n",
    "top_importances = importances[:1]\n",
    "# bottom_features = features[-10:]\n",
    "# bottom_importances = importances[-10:]\n",
    "\n",
    "# Combine top and bottom features for plotting\n",
    "plot_features = top_features #+ bottom_features\n",
    "plot_importances = top_importances #+ #bottom_importances\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(plot_features, plot_importances, color='skyblue')\n",
    "plt.xlabel('Average Coefficient Value')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('feat_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum all confusion matrices to get overall counts\n",
    "overall_cf_matrix = np.sum(cf_cv, axis=0)\n",
    "print(\"Overall Confusion Matrix:\\n\", overall_cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Create a heatmap to visualize the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(overall_cf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Overall Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('overall_confusion_matrix.png')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running this check for keywords\n",
    "# Define custom feature names\n",
    "custom_plot_features = [\n",
    "    \"ICD: 332.0 / G20\"\n",
    "]\n",
    "print(len(custom_plot_features)) \n",
    "print(len(plot_features)) \n",
    "# Ensure the length of custom features matches the length of plot_features\n",
    "assert len(custom_plot_features) == len(plot_features), \"The number of custom features must match the number of plot features.\"\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(plot_features)), plot_importances, color='skyblue')\n",
    "plt.xlabel('Average Coefficient Value')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "\n",
    "# Set custom y-axis labels\n",
    "plt.yticks(range(len(plot_features)), custom_plot_features)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(cf_matrix):\n",
    "    TN, FP, FN, TP = cf_matrix.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Calculate metrics for each fold\n",
    "for fold_index, cf_matrix in enumerate(cf_cv):\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(cf_matrix)\n",
    "    print(f\"Metrics for Fold {fold_index + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_cf_matrix = np.sum(cf_cv, axis=0)\n",
    "overall_accuracy, overall_precision, overall_recall, overall_f1 = calculate_metrics(overall_cf_matrix)\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Precision: {overall_precision:.4f}\")\n",
    "print(f\"Recall: {overall_recall:.4f}\")\n",
    "print(f\"F1 Score: {overall_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_row_numbers))\n",
    "print(len(all_true_labels))\n",
    "print(len(all_predictions))\n",
    "print(len(all_predictions_nonbin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves overlayed on a single graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "for fpr, tpr, auc_score in roc_curves:\n",
    "    plt.plot(fpr, tpr, label='AUC = {:.4f}'.format(auc_score))\n",
    "    youden = np.max(tpr-fpr)\n",
    "    plt.scatter(fpr[np.argmax(tpr-fpr)], tpr[np.argmax(tpr-fpr)], c='red', label=f'Youden\\'s Index = {youden:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classifier\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('AUC_iter.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot all PR curves overlayed on a single graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "for recall, precision, auc_pr_loop in pr_curves:\n",
    "    plt.plot(recall, precision, label='PR Curve (AUC = {:.4f})'.format(auc_pr_loop))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig('PR_iter.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_df = pd.DataFrame({'Unnamed: 0': all_row_numbers, 'annot': all_true_labels, 'prediction BINARY': all_predictions, 'prediction': all_predictions_nonbin})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('LR_iter_pred_rr.csv', index=False)\n",
    "\n",
    "# Calculate the final AUC and F1\n",
    "auc_final = np.mean(auc_cv)\n",
    "f1_final = np.mean(f1_cv)\n",
    "\n",
    "# Save all the data\n",
    "df = pd.DataFrame()\n",
    "df['auc'] = auc_cv\n",
    "df['f1s'] = f1_cv\n",
    "df['auc_pr'] = auc_pr\n",
    "df_pred = pd.DataFrame()\n",
    "df1 = pd.DataFrame(predictions[0])\n",
    "df2 = pd.DataFrame(predictions[1])\n",
    "df3 = pd.DataFrame(predictions[2])\n",
    "df4 = pd.DataFrame(predictions[3])\n",
    "df5 = pd.DataFrame(predictions[4])\n",
    "df_pred = pd.concat([df1,df2,df3,df4,df5]).reset_index()\n",
    "df.to_csv('LR_iter.csv')\n",
    "\n",
    "# Print final Data\n",
    "print(auc_final)\n",
    "print(f1_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bramenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# final code\n",
    "# df=pd.read_csv('/home/bram/bramenv/afeaturematrix/ALLHOSPITALS_DATA.csv')\n",
    "df=pd.read_csv('/home/bram/bramenv/afeaturematrix/ALLHOSPITALS_NEW_DATA.csv')\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "print(df['BDSPPatientID'].nunique())\n",
    "\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "print(df)\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Ensure you have the necessary nltk resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "keywords = [\n",
    "    \"parkinson disease\", \"pd\", \"parkinson\", 'parkinsonismx', \"disease\", \"paralysis agitans\", \"primary parkinsonism\", \n",
    "    \"pdd\", \"akathisia\", \"alien limb\", \"alpha synuclein\", \"anosmia\", \"anticholinergic side effect\", \n",
    "    \"apraxia\", \"autonomic\", \"axial\", \"basophilic\", \"blepharospasm\", \"bradyphasia\", \"bradyphonia\", \n",
    "    \"bradyphrenia\", \"camptocormia\", \"cholinergic\", \"cogwheel\", \"cognitive impairment\", \"corticobasal\", \n",
    "    \"daws\", \"denervation\", \"dopamine\", \"dopamine agonist\", \"dysphagia\", \"dysphonia\", \"dystonia\", \n",
    "    \"facial\", \"fasciculation\", \"gba\", \"gait\", \"festinating\", \"shuffling\", \"freezing\", \"globus\", \n",
    "    \"hyposmia\", \"hyperreflexia\", \"hypokinetic\", \"hypomimia\", \"hypophonia\", \"idiopathic\", \"imaging\", \n",
    "    \"dat spect\", \"datscan\", \"dat\", \"mri\", \"scan\", \"lrrk2\", \"snca\", \"prkn\", \"pink1\", \"dj-1\", \"gba\", \n",
    "    \"vps35\", \"atp13a2\", \"lrrk1\", \"uchl1\", \"mao-b\", \"monoamine oxidase b inhibitor\", \"masked facies\", \n",
    "    \"medullary\", \"micrographia\", \"mild cognitive impairment\", \"mci\", \"montreal cognitive assessment\", \n",
    "    \"moca\", \"movement disorder\", \"multiple system atrophy\", \"msa\", \"nigra pars compacta\", \"nigrostriatal\", \n",
    "    \"ocular torticollis\", \"occupational therapy\", \"orthostatic\", \"orofacial\", \"pallidotomy\", \n",
    "    \"pallidothalamic\", \"palliative\", \"paratonia\", \"pathology\", \"pet\", \"physical therapy\", \"postencephalitic\", \n",
    "    \"posture\", \"primary progressive aphasia\", \"ppa\", \"prodrome\", \"progressive supranuclear palsy\", \n",
    "    \"psp\", \"pyramidal\", \"rem sleep behavior disorder\", \"rbd\", \"restless legs\", \"saccade\", \"sialorrhea\", \n",
    "    \"speech therapy\", \"spasmodic\", \"stooped\", \"subthalamic nucleus\", \"stn\", \"striatal dopamine deficiency\", \n",
    "    \"stiff\", \"rigidity\", \"subcortical\", \"supranuclear\", \"swallowing\", \"tetany\", \"thalamotomy\", \"tics\", \"toronto cognitive assessment\", \"torca\", \"tremor\", \"uptake\", \"vascular\", \"nausea\", \n",
    "    \"hallucination\", \"constipation\", \"fatigue\", \"insomnia\", \"depression\", \"anxiety\", \"psychosis\", \n",
    "    \"dosage adjustment\", \"physiotherapy\", \"lifestyle changes\", \"support group\", \"caregiver\", \n",
    "    \"medication adherence\", \"prescription refill\", \"fall precaution\", \"daily function\", \"self care\", \n",
    "    \"quality of life\", \"disease progression\", \"regular follow-up\", \"exercise\", \"sleep hygiene\", \n",
    "    \"bone health\", \"care coordination\", \"shuffling gait\", \"exelon patch\", \"exelon\", \"cane\", \"fall\", \n",
    "    \"deep brain stimulation\", \"dbs\", \"bradykinesia\", \"hypokinesia\", \"dementia\", \"pulmonary complication\", \n",
    "    \"pneumonia\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to stem each word in a phrase\n",
    "def stem_phrase(phrase, stemmer):\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Negative Words and Stem Them\n",
    "# negative_words_before = [\"no\", \"not\", \"n't\", \"absent\", \"negative\"]\n",
    "# stemmed_negative_words_before = [stemmer.stem(word) for word in negative_words_before]\n",
    "# negative_words_after = [\"absent\", \"negative\"]\n",
    "# stemmed_negative_words_after = [stemmer.stem(word) for word in negative_words_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate regex patterns\n",
    "# def generate_patterns(keywords, stemmed_negative_words_before, stemmed_negative_words_after):\n",
    "#     neg_patterns = {}\n",
    "#     pos_patterns = {}\n",
    "#     for phrase in keywords:\n",
    "#         stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "#         words = stemmed_phrase.split()\n",
    "#         neg_pattern = r\"\\b\" + r\"(?:(?:\" + '|'.join(stemmed_negative_words_before) + r\")[^,.]{{0,30}}{pattern})|(?:{pattern}[^.]{{0,80}}(?:\" + '|'.join(stemmed_negative_words_after) + r\"))\".format(\n",
    "#             pattern= r''.join([f\"{word}\\s*\" for word in words]) + r\"\\b\"\n",
    "#         )\n",
    "#         neg_patterns[phrase] = neg_pattern\n",
    "#         pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) + r\"\\b\"\n",
    "#     return neg_patterns, pos_patterns\n",
    "\n",
    "# neg_patterns, pos_patterns = generate_patterns(keywords, stemmed_negative_words_before, stemmed_negative_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1# Generate regex patterns\n",
    "def generate_patterns(keywords):\n",
    "    neg_patterns = {}\n",
    "    pos_patterns = {}\n",
    "    for phrase in keywords:\n",
    "        stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "        words = stemmed_phrase.split()\n",
    "        neg_pattern = r\"(?:(?:\\bno\\b|\\bnot\\b|n't|absent|negat)[^,.]{{0,30}}\\b{pattern})\\b|\\b(?:{pattern}\\b[^.]{{0,40}}(?:absent|negat))\".format(\n",
    "            pattern= r''.join([f\"{word}\\s*\" for word in words]) #+ r\"\\b\"\n",
    "        )\n",
    "        neg_patterns[phrase] = neg_pattern\n",
    "        pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) + r\"\\b\" \n",
    "    return neg_patterns, pos_patterns\n",
    "\n",
    "neg_patterns, pos_patterns = generate_patterns(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_patterns(keywords):\n",
    "#     neg_patterns = {}\n",
    "#     pos_patterns = {}\n",
    "#     for phrase in keywords:\n",
    "#         stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "#         words = stemmed_phrase.split()\n",
    "#         neg_pattern = r\"\\b\" + r\"(?:(?:no|not|n't|absent|negative)[^,.]{{0,30}}{pattern})|(?:{pattern}[^.]{{0,80}}(?:absent|negative))\".format(\n",
    "#             pattern= r''.join([f\"{word}\\s*\" for word in words]) +r\"\\b\"\n",
    "#         )\n",
    "#         neg_patterns[phrase] = neg_pattern\n",
    "#         pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) +r\"\\b\"\n",
    "#     return neg_patterns, pos_patterns\n",
    "\n",
    "# neg_patterns, pos_patterns = generate_patterns(keywords)\n",
    "\n",
    "# Print generated patterns for verification\n",
    "print(\"Negated Patterns:\")\n",
    "for key, pattern in neg_patterns.items():\n",
    "    print(f\"{key}: {pattern}\")\n",
    "\n",
    "print(\"\\nPositive Patterns:\")\n",
    "for key, pattern in pos_patterns.items():\n",
    "    print(f\"{key}: {pattern}\")\n",
    "\n",
    "# # Function to preprocess text (lowercase, stemming using NLTK)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r\"\\b[a-z]*parkinsonism[a-z]*\\b\", \"parkinsonismx\", text)  # Replace words containing 'parkinsonism'\n",
    "    # print (text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token != \"'s\"]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "# no|not|n't|absent|negat\n",
    "# absent|negat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_patterns(text, neg_patterns, pos_patterns):\n",
    "    matches = {f\"{key}_pos\": 0 for key in pos_patterns}\n",
    "    matches.update({f\"{key}_neg\": 0 for key in neg_patterns})\n",
    "\n",
    "    for key in pos_patterns:\n",
    "        pos_matches = [(m.start(), m.end()) for m in re.finditer(pos_patterns[key], text)]\n",
    "        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key], text)]\n",
    "\n",
    "        for start, end in pos_matches:\n",
    "            if not any(n_start <= start <= n_end or n_start <= end <= n_end for n_start, n_end in neg_matches):\n",
    "                matches[f\"{key}_pos\"] = 1\n",
    "\n",
    "        for start, end in neg_matches:\n",
    "            matches[f\"{key}_neg\"] = 1\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing and pattern checking to each note\n",
    "df['processed_note'] = df['note_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed notes for verification\n",
    "print(\"Preprocessed Notes:\")\n",
    "print(df['processed_note'])\n",
    "\n",
    "pattern_features = df['processed_note'].apply(lambda note_text: check_patterns(note_text, neg_patterns, pos_patterns))\n",
    "\n",
    "# Convert the pattern features to a DataFrame\n",
    "pattern_columns = pd.DataFrame(pattern_features.tolist())\n",
    "\n",
    "# Concatenate with the original DataFrame\n",
    "result = pd.concat([df[['BDSPPatientID', 'CreateDate', 'hospital']], pattern_columns], axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv('FEATMATRIX_TEXT_NEWEST2.csv', index=False)\n",
    "result.to_csv('FEATMATRIX_TEXT_test3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old=pd.read_csv('/home/bram/bramenv/aa_test matrix/FEATMATRIX_TEXT_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old['parkinson disease_pos'].sum())\n",
    "print(old[\"parkinson's disease_pos\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['parkinson disease_pos'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "import re\n",
    "\n",
    "def stem_phrase(phrase, stemmer):\n",
    "    # Dummy stem_phrase function, replace with actual stemming logic\n",
    "    return phrase\n",
    "\n",
    "# Define a dummy stemmer, replace with actual stemmer\n",
    "stemmer = None\n",
    "\n",
    "def generate_patterns(keywords):\n",
    "    neg_patterns = {}\n",
    "    pos_patterns = {}\n",
    "    \n",
    "    for phrase in keywords:\n",
    "        stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "        words = stemmed_phrase.split()\n",
    "\n",
    "        # Create a pattern to detect negated words\n",
    "        neg_pattern = (\n",
    "            r\"\\b(?:(?:no|not|n't|absent|negative)[^,.]{{0,30}}{pattern})|\"\n",
    "            r\"(?:{pattern}[^.]{{0,80}}(?:absent|negative))\"\n",
    "        ).format(pattern=r''.join([f\"{word}\\\\s*\" for word in words]) + r\"\\b\")\n",
    "\n",
    "        # Create a pattern to detect non-negated words\n",
    "        non_negated_pattern = (\n",
    "            r\"(?:(?:(?!\\b(?:no|not|n't|absent|negative)\\b)[^,.]{{0,30}}{pattern})|\"\n",
    "            r\"(?:{pattern}[^.]{{0,80}}(?!\\b(?:absent|negative)\\b)))\"\n",
    "        ).format(pattern=r''.join([f\"{word}\\\\s*\" for word in words]) + r\"\\b\")\n",
    "\n",
    "        neg_patterns[phrase] = neg_pattern\n",
    "        pos_patterns[phrase] = non_negated_pattern\n",
    "    \n",
    "    return neg_patterns, pos_patterns\n",
    "\n",
    "# Example usage\n",
    "keywords = [\"happy\", \"joyful\"]\n",
    "neg_patterns, pos_patterns = generate_patterns(keywords)\n",
    "\n",
    "text = \"I am not happy about this. This is a joyful moment. The absence of joy is felt.\"\n",
    "\n",
    "# Test the generated patterns\n",
    "for phrase in keywords:\n",
    "    neg_matches = re.findall(neg_patterns[phrase], text, re.IGNORECASE)\n",
    "    pos_matches = re.findall(pos_patterns[phrase], text, re.IGNORECASE)\n",
    "    print(f\"Negated matches for '{phrase}': {neg_matches}\")\n",
    "    print(f\"Non-negated matches for '{phrase}': {pos_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def stem_phrase(phrase, stemmer):\n",
    "#     # Dummy stem_phrase function, replace with actual stemming logic\n",
    "#     return phrase\n",
    "\n",
    "# # Define a dummy stemmer, replace with actual stemmer\n",
    "# stemmer = None\n",
    "\n",
    "def generate_patterns(keywords):\n",
    "    neg_patterns = {}\n",
    "    pos_patterns = {}\n",
    "    \n",
    "    for phrase in keywords:\n",
    "        stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "        words = stemmed_phrase.split()\n",
    "\n",
    "        word_pattern = r'\\b' + r'\\s*'.join([f\"{word}\" for word in words]) + r'\\b'\n",
    "        \n",
    "        # Create a pattern to detect negated words\n",
    "        neg_pattern = (\n",
    "            rf\"\\b(?:(?:no|not|n't|absent|negative)[^,.]{{0,30}}{word_pattern})|\"\n",
    "            rf\"(?:{word_pattern}[^.]{{0,80}}(?:absent|negative))\"\n",
    "        )\n",
    "\n",
    "        # Create a pattern to detect non-negated words\n",
    "        # non_negated_pattern = (\n",
    "        #     rf\"\\b(?:(?!(?:no|not|n't|absent|negative))[^,.]{{0,30}}{word_pattern})|\"\n",
    "        #     rf\"(?:{word_pattern}[^.]{{0,80}}(?!(?:absent|negative)))\"\n",
    "        # )\n",
    "        # non_negated_pattern = (\n",
    "        #     rf\"\\b(?:(?!no|not|n't|absent|negative)[^,.]{{0,30}}{word_pattern})|\"\n",
    "        #     rf\"(?:{word_pattern}[^.]{{0,80}}(?!absent|negative))\"\n",
    "        non_negated_pattern = (\n",
    "            r\"(?:(?:(?!\\b(?:no|not|n't|absent|negative)\\b)[^,.]{{0,30}}{pattern})|\"\n",
    "            r\"(?:{pattern}[^.]{{0,80}}(?!\\b(?:absent|negative)\\b)))\"\n",
    "        )\n",
    "\n",
    "\n",
    "        neg_patterns[phrase] = neg_pattern\n",
    "        pos_patterns[phrase] = non_negated_pattern\n",
    "    \n",
    "    return neg_patterns, pos_patterns\n",
    "\n",
    "# Example usage\n",
    "keywords = [\"happy\", \"joyful\", 'hero']\n",
    "neg_patterns, pos_patterns = generate_patterns(keywords)\n",
    "\n",
    "text = \"I am not happy about this. This is a joyful moment. The absence of joy is felt. spiderman is my hero. spiderman is not my hero\"\n",
    "\n",
    "# Test the generated patterns\n",
    "for phrase in keywords:\n",
    "    neg_matches = re.findall(neg_patterns[phrase], text, re.IGNORECASE)\n",
    "    pos_matches = re.findall(pos_patterns[phrase], text, re.IGNORECASE)\n",
    "    print(f\"Negated matches for '{phrase}': {neg_matches}\")\n",
    "    print(f\"Non-negated matches for '{phrase}': {pos_matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Read the CSV file (Replace this with test data for now)\n",
    "data = {\n",
    "    \"BDSPPatientID\": [1, 2, 3, 4, 5],\n",
    "    \"CreateDate\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\"],\n",
    "    \"hospital\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "    \"note\": [\n",
    "        \"The patient shows signs of parkinson disease and no evidence of other injurieS but later on appers to have no parkinson\",\n",
    "        \"No parkinson disease was observed, but signs of hypoxic injury were present.\",\n",
    "        \"The patient did not show any signs of parkinsonism, however, there were indications of cognitive impairment.\",\n",
    "        \"There is evidence of parkinsonism and cognitive impairment.\",\n",
    "        \"The report mentioned no parkinson disease or hypoxic injury.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataframe details\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Number of unique patient IDs: {df['BDSPPatientID'].nunique()}\")\n",
    "\n",
    "# Setting Up NLTK\n",
    "nltk.download('punkt')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Define Keywords\n",
    "keywords = [\n",
    "    \"parkinson disease\", \"pd\", \"parkinson\", \"hypoxic injury\", \"cognitive impairment\", \"parkinsonism\", 'injuries'\n",
    "]\n",
    "\n",
    "# Function to stem each word in a phrase\n",
    "def stem_phrase(phrase, stemmer):\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens])\n",
    "\n",
    "# Generate regex patterns\n",
    "def generate_patterns(keywords):\n",
    "    neg_patterns = {}\n",
    "    pos_patterns = {}\n",
    "    for phrase in keywords:\n",
    "        stemmed_phrase = stem_phrase(phrase, stemmer)\n",
    "        words = stemmed_phrase.split()\n",
    "        neg_pattern = r\"\\b\" + r\"(?:(?:no|not|n't|absent|negative)[^,.]{{0,30}}{pattern})|(?:{pattern}[^.]{{0,40}}(?:absent|negative))\".format(\n",
    "            pattern= r''.join([f\"{word}\\s*\" for word in words]) + r\"\\b\"\n",
    "        )\n",
    "        neg_patterns[phrase] = neg_pattern\n",
    "        pos_patterns[phrase] = r\"\\b\" + r\"\".join([f\"{word}\\s*\" for word in words]) + r\"\\b\"\n",
    "    return neg_patterns, pos_patterns\n",
    "\n",
    "neg_patterns, pos_patterns = generate_patterns(keywords)\n",
    "\n",
    "# Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r\"\\b[a-z]*parkinsonism[a-z]*\\b\", \"parkinsonismx\", text)  # Replace words containing 'parkinsonism'\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Check Patterns\n",
    "def check_patterns(text, neg_patterns, pos_patterns):\n",
    "    matches = {f\"{key}_pos\": 0 for key in pos_patterns}\n",
    "    matches.update({f\"{key}_neg\": 0 for key in neg_patterns})\n",
    "\n",
    "    for key in pos_patterns:\n",
    "        # Find all positions of positive matches\n",
    "        pos_matches = [(m.start(), m.end()) for m in re.finditer(pos_patterns[key], text)]\n",
    "        # Find all positions of negated matches\n",
    "        neg_matches = [(m.start(), m.end()) for m in re.finditer(neg_patterns[key], text)]\n",
    "\n",
    "        # Check if positive match is not within any negated match\n",
    "        for start, end in pos_matches:\n",
    "            if not any(n_start <= start <= n_end or n_start <= end <= n_end for n_start, n_end in neg_matches):\n",
    "                matches[f\"{key}_pos\"] = 1\n",
    "\n",
    "        # Set negated matches\n",
    "        for start, end in neg_matches:\n",
    "            matches[f\"{key}_neg\"] = 1\n",
    "\n",
    "    return matches\n",
    "\n",
    "# Apply preprocessing and pattern checking\n",
    "df['processed_note'] = df['note'].apply(preprocess_text)\n",
    "\n",
    "pattern_features = df['processed_note'].apply(lambda note_text: check_patterns(note_text, neg_patterns, pos_patterns))\n",
    "\n",
    "# Convert the pattern features to a DataFrame\n",
    "pattern_columns = pd.DataFrame(pattern_features.tolist())\n",
    "\n",
    "# Concatenate with the original DataFrame\n",
    "result = pd.concat([df[['BDSPPatientID', 'CreateDate', 'hospital']], pattern_columns], axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "# result.to_csv('FEATMATRIX_TEXT.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bramenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

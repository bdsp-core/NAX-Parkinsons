{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics and other results\n",
    "# X_holdout = pd.read_csv('path_to_holdout_features.csv')  # replace with actual path?\n",
    "matrix= pd.read_csv('/home/bram/bramenv/aa_test_2 matrix/test_set_final.csv')\n",
    "\n",
    "# Extract the 'BDSPPatientID' and 'annot' columns to create y_data_pre\n",
    "y_data_pre = matrix[['BDSPPatientID', 'annot']]\n",
    "y_holdout = y_data_pre['annot']\n",
    "X_holdout = matrix.drop(['BDSPPatientID', 'annot','CreateDate', 'hospital'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics and other results\n",
    "auc_holdout = []\n",
    "f1_holdout = []\n",
    "precision_holdout = []\n",
    "recall_holdout = []\n",
    "auc_pr_holdout = []\n",
    "cf_holdout = []\n",
    "holdout_predictions = []\n",
    "roc_curves_holdout = []\n",
    "pr_curves_holdout = []\n",
    "patient_ids = y_data_pre['BDSPPatientID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store false positive and false negative patient IDs\n",
    "false_positives = []\n",
    "false_negatives = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store feature importances\n",
    "feature_importances_dict = {feature: [] for feature in X_holdout.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred_proba = 0\n",
    "y_pred = 0\n",
    "\n",
    "for fold in range(10):\n",
    "    model_filename = f'/home/bram/bramenv/aa_test_2 matrix/model_RF/model_train_allhospitals_Notes+ICD+Med_fold{fold+1}.pickle'  # Updated path\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    model = res['model']\n",
    "    cutoff = res['cutoff']\n",
    "    \n",
    "    # Store patient IDs\n",
    "   \n",
    "    \n",
    "   \n",
    "    yp =  model.predict_proba(X_holdout)[:, 1]\n",
    "\n",
    "    # for probability output, we take average as the final probability\n",
    "    y_pred_proba += yp\n",
    "    \n",
    "    # for binary output (yes/no), we binarize within each fold, and then take majority vote across folds to get final binary output\n",
    "    y_pred += (yp>cutoff).astype(int)\n",
    "\n",
    "y_pred_proba /= 10\n",
    "y_pred_bin = (y_pred>5).astype(int)\n",
    "\n",
    "fpr, tpr, cutoffs = roc_curve(y_holdout, y_pred_proba)\n",
    "\n",
    "auc_holdout=roc_auc_score(y_holdout, y_pred_proba)\n",
    "f1_holdout=f1_score(y_holdout, y_pred_bin)\n",
    "precision_holdout=precision_score(y_holdout, y_pred_bin)\n",
    "recall_holdout=recall_score(y_holdout, y_pred_bin)\n",
    "accuracy_holdout = accuracy_score(y_holdout, y_pred_bin)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_holdout, y_pred_proba)\n",
    "auc_pr = auc(recall, precision)\n",
    "# auc_pr_holdout.append(auc_pr)\n",
    "\n",
    "cf_holdout=confusion_matrix(y_holdout, y_pred_bin)\n",
    "holdout_predictions=y_pred_bin\n",
    "\n",
    "roc_curves_holdout.append((fpr, tpr, roc_auc_score(y_holdout, y_pred_proba)))\n",
    "pr_curves_holdout.append((recall, precision, auc_pr))\n",
    "\n",
    "# Identify false positives and false negatives\n",
    "false_positive_ids = patient_ids[(y_holdout == 0) & (y_pred_bin == 1)]\n",
    "false_negative_ids = patient_ids[(y_holdout == 1) & (y_pred_bin == 0)]\n",
    "false_positives.append(false_positive_ids)\n",
    "false_negatives.append(false_negative_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_bin)\n",
    "print(y_pred_proba)\n",
    "print(len(y_pred_bin))\n",
    "print(len(y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cf_holdout, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Overall Confusion Matrix on Holdout Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.savefig('overall_confusion_matrix_holdout.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(cf_matrix):\n",
    "    TN, FP, FN, TP = cf_matrix.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics on the holdout set\n",
    "overall_accuracy_holdout, overall_precision_holdout, overall_recall_holdout, overall_f1_holdout = calculate_metrics(cf_holdout)\n",
    "print(\"Overall Metrics on Holdout Set:\")\n",
    "print(f\"Accuracy: {overall_accuracy_holdout:.4f}\")\n",
    "print(f\"Precision: {overall_precision_holdout:.4f}\")\n",
    "print(f\"Recall: {overall_recall_holdout:.4f}\")\n",
    "print(f\"F1 Score: {overall_f1_holdout:.4f}\")\n",
    "\n",
    "# Plot all ROC curves overlayed on a single graph for holdout set\n",
    "plt.figure(figsize=(8, 6))\n",
    "for fpr, tpr, auc_score in roc_curves_holdout:\n",
    "    plt.plot(fpr, tpr, label='AUC = {:.4f}'.format(auc_score))\n",
    "    # youden = np.max(tpr - fpr)\n",
    "    plt.scatter(fpr[np.argmax(tpr - fpr)], tpr[np.argmax(tpr - fpr)], c='red')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve on Holdout Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('AUC_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot all PR curves overlayed on a single graph for holdout set\n",
    "plt.figure(figsize=(8, 6))\n",
    "for recall, precision, auc_pr_loop in pr_curves_holdout:\n",
    "    plt.plot(recall, precision, label='PR Curve (AUC = {:.4f})'.format(auc_pr_loop))\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve on Holdout Set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig('PR_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results_holdout_df = pd.DataFrame({\n",
    "    'BDSPPatientID': y_data_pre['BDSPPatientID'],\n",
    "    'true_label': y_holdout,\n",
    "    'prediction': y_pred_proba,\n",
    "    'prediction binary': y_pred_bin\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_holdout_df.to_csv('LR_holdout_pred.csv', index=False)\n",
    "\n",
    "# Save false positives and false negatives to CSV files\n",
    "false_positives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fp) for fp in false_positives]), 'BDSPPatientID': np.concatenate(false_positives)})\n",
    "false_negatives_df = pd.DataFrame({'Fold': np.repeat(range(1), [len(fn) for fn in false_negatives]), 'BDSPPatientID': np.concatenate(false_negatives)})\n",
    "\n",
    "false_positives_df.to_csv('false_positives.csv', index=False)\n",
    "false_negatives_df.to_csv('false_negatives.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resample(y_true, y_pred_proba, y_pred_bin, n_iterations=1000, alpha=0.95):\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "    auc_prs = []\n",
    "    roc_curves = []\n",
    "    pr_curves = []\n",
    "    \n",
    "    n_size = len(y_true)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.randint(0, n_size, n_size)\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue  # skip this resample if it does not have both classes\n",
    "        \n",
    "        aucs.append(roc_auc_score(y_true[indices], y_pred_proba[indices]))\n",
    "        f1s.append(f1_score(y_true[indices], y_pred_bin[indices]))\n",
    "        precisions.append(precision_score(y_true[indices], y_pred_bin[indices]))\n",
    "        recalls.append(recall_score(y_true[indices], y_pred_bin[indices]))\n",
    "        accuracies.append(accuracy_score(y_true[indices], y_pred_bin[indices]))\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_true[indices], y_pred_proba[indices])\n",
    "        auc_prs.append(auc(recall, precision))\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_pred_proba[indices])\n",
    "        roc_curves.append((fpr, tpr))\n",
    "        \n",
    "        pr_curves.append((precision, recall))\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_p = ((1.0 - alpha) / 2.0) * 100\n",
    "    upper_p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n",
    "    \n",
    "    auc_ci = np.percentile(aucs, [lower_p, upper_p])\n",
    "    f1_ci = np.percentile(f1s, [lower_p, upper_p])\n",
    "    precision_ci = np.percentile(precisions, [lower_p, upper_p])\n",
    "    recall_ci = np.percentile(recalls, [lower_p, upper_p])\n",
    "    accuracy_ci = np.percentile(accuracies, [lower_p, upper_p])\n",
    "    auc_pr_ci = np.percentile(auc_prs, [lower_p, upper_p])\n",
    "    \n",
    "    return auc_ci, f1_ci, precision_ci, recall_ci, accuracy_ci, auc_pr_ci, roc_curves, pr_curves\n",
    "\n",
    "# Perform bootstrap resampling to get confidence intervals\n",
    "auc_ci, f1_ci, precision_ci, recall_ci, accuracy_ci, auc_pr_ci, roc_curves_bootstrap, pr_curves_bootstrap = bootstrap_resample(y_holdout, y_pred_proba, y_pred_bin)\n",
    "\n",
    "# Print metrics and confidence intervals\n",
    "print(\"Metrics on Holdout Set:\")\n",
    "print(f\"AUC: {auc_holdout:.4f} (95% CI: {auc_ci[0]:.4f} - {auc_ci[1]:.4f})\")\n",
    "print(f\"F1 Score: {f1_holdout:.4f} (95% CI: {f1_ci[0]:.4f} - {f1_ci[1]:.4f})\")\n",
    "print(f\"Precision: {precision_holdout:.4f} (95% CI: {precision_ci[0]:.4f} - {precision_ci[1]:.4f})\")\n",
    "print(f\"Recall: {recall_holdout:.4f} (95% CI: {recall_ci[0]:.4f} - {recall_ci[1]:.4f})\")\n",
    "print(f\"Accuracy: {accuracy_holdout:.4f} (95% CI: {accuracy_ci[0]:.4f} - {accuracy_ci[1]:.4f})\")\n",
    "print(f\"AUC-PR: {auc_pr:.4f} (95% CI: {auc_pr_ci[0]:.4f} - {auc_pr_ci[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_intervals(curves, n_bins=100):\n",
    "    mean_curve = np.linspace(0, 1, n_bins)\n",
    "    interpolated_curves = [np.interp(mean_curve, curve[0], curve[1]) for curve in curves]\n",
    "    lower_ci = np.percentile(interpolated_curves, 2.5, axis=0)\n",
    "    upper_ci = np.percentile(interpolated_curves, 97.5, axis=0)\n",
    "    mean_curve = np.mean(interpolated_curves, axis=0)\n",
    "    return mean_curve, lower_ci, upper_ci\n",
    "\n",
    "# Calculate confidence intervals for ROC\n",
    "roc_fpr_interp = np.linspace(0, 1, 100)\n",
    "roc_tpr_interp = [np.interp(roc_fpr_interp, fpr, tpr) for fpr, tpr in roc_curves_bootstrap]\n",
    "roc_mean_tpr = np.mean(roc_tpr_interp, axis=0)\n",
    "roc_lower_tpr = np.percentile(roc_tpr_interp, 2.5, axis=0)\n",
    "roc_upper_tpr = np.percentile(roc_tpr_interp, 97.5, axis=0)\n",
    "\n",
    "# Calculate confidence intervals for PR\n",
    "pr_recall_interp = np.linspace(0, 1, 100)\n",
    "pr_precision_interp = [np.interp(pr_recall_interp, recall[::-1], precision[::-1]) for precision, recall in pr_curves_bootstrap]\n",
    "pr_mean_precision = np.mean(pr_precision_interp, axis=0)\n",
    "pr_lower_precision = np.percentile(pr_precision_interp, 2.5, axis=0)\n",
    "pr_upper_precision = np.percentile(pr_precision_interp, 97.5, axis=0)\n",
    "\n",
    "# Plot ROC curve with confidence intervals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(roc_curves_holdout[0][0], roc_curves_holdout[0][1], label=f'Main ROC (AUC = {roc_curves_holdout[0][2]:.4f})', color='blue')\n",
    "plt.fill_between(roc_fpr_interp, roc_lower_tpr, roc_upper_tpr, color='blue', alpha=0.2, label='95% CI')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve on Holdout Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('AUC_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot PR curve with confidence intervals\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(pr_curves_holdout[0][0], pr_curves_holdout[0][1], label=f'Main PR Curve (AUC = {pr_curves_holdout[0][2]:.4f})', color='blue')\n",
    "plt.fill_between(pr_recall_interp, pr_lower_precision, pr_upper_precision, color='blue', alpha=0.2, label='95% CI')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve on Holdout Set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig('PR_holdout.png')\n",
    "plt.show()\n",
    "\n",
    "# Save all the data\n",
    "df_holdout = pd.DataFrame({\n",
    "    'metric': ['auc', 'f1', 'precision', 'recall', 'accuracy', 'auc_pr'],\n",
    "    'value': [auc_holdout, f1_holdout, precision_holdout, recall_holdout, accuracy_holdout, auc_pr],\n",
    "    'ci_lower': [auc_ci[0], f1_ci[0], precision_ci[0], recall_ci[0], accuracy_ci[0], auc_pr_ci[0]],\n",
    "    'ci_upper': [auc_ci[1], f1_ci[1], precision_ci[1], recall_ci[1], accuracy_ci[1], auc_pr_ci[1]]\n",
    "})\n",
    "df_holdout.to_csv('LR_holdout_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bramenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
